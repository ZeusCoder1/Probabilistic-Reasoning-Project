{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <h1 style=\"color:yellow;\">MCDO-NN</h1>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MCDoNN Model Evaluation Metrics\n",
      "==================================================\n",
      "Overall RMSE: 1017.06\n",
      "Overall MAE: 600.33\n",
      "Overall R²: -0.520\n",
      "\n",
      "Predictions for 2023-2024:\n",
      "--------------------------------------------------\n",
      "Commodity                      Prediction  Uncertainty\n",
      "--------------------------------------------------\n",
      "Rice - Basmati                       9.42 ±      1.75\n",
      "Rice (Other than Basmati)           11.42 ±      2.29\n",
      "Wheat                                5.28 ±      1.32\n",
      "Other Cereals                        2.70 ±      1.07\n",
      "Pulses                              -2.18 ±      1.41\n",
      "Tobacco Unmanufactured              -0.00 ±      0.92\n",
      "Tobacco Manufactured                -2.10 ±      1.44\n",
      "Cashew                              -1.63 ±      1.28\n",
      "Cashew Nut Shell Liquid             -3.99 ±      2.04\n",
      "Sesame Seeds                        -1.42 ±      1.27\n",
      "Niger Seeds                         -3.81 ±      1.94\n",
      "Groundnut                           -0.50 ±      1.01\n",
      "Other Oil Seeds                     -4.06 ±      1.88\n",
      "Vegetable Oils                      -2.02 ±      1.70\n",
      "Oil Meals                            4.10 ±      1.03\n",
      "Guar Gum Meal                       -2.04 ±      1.66\n",
      "Castor Oil                           5.17 ±      1.02\n",
      "Shellac                             -3.49 ±      1.72\n",
      "Sugar                               10.03 ±      1.76\n",
      "Molasses                            -2.73 ±      1.75\n",
      "Fruits / Vegetable Seeds            -3.31 ±      1.58\n",
      "Fresh Fruits                        -0.07 ±      1.01\n",
      "Fresh Vegetables                     3.67 ±      1.01\n",
      "Processed Vegetables                -0.58 ±      1.22\n",
      "Processed Fruits and Juices          1.69 ±      0.92\n",
      "Cereal Preparations                  0.55 ±      0.88\n",
      "\n",
      "Visualization files have been saved:\n",
      "1. mcdonn_predictions.png - Shows predictions with uncertainty bands\n",
      "2. mcdonn_evaluation.png - Shows evaluation metrics and uncertainty analysis\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import math\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "class MCDoNN:\n",
    "    def __init__(self, input_dim, dropout_rate=0.2, hidden_layers=[64, 32, 16]):\n",
    "        self.input_dim = input_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.model = self._build_model()\n",
    "        self.scaler = MinMaxScaler()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        inputs = Input(shape=(self.input_dim,))\n",
    "        x = inputs\n",
    "        \n",
    "        # Hidden layers with dropout and batch normalization\n",
    "        for units in self.hidden_layers:\n",
    "            x = Dense(units, activation='relu')(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Dropout(self.dropout_rate)(x)\n",
    "        \n",
    "        # Output layer\n",
    "        outputs = Dense(1)(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y, validation_split=0.2, epochs=200, batch_size=32):\n",
    "        # Scale the data\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Early stopping callback\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        history = self.model.fit(\n",
    "            X_scaled, y,\n",
    "            validation_split=validation_split,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict_with_uncertainty(self, X, n_iterations=100):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        predictions = []\n",
    "        \n",
    "        # Enable dropout at inference time\n",
    "        for _ in range(n_iterations):\n",
    "            pred = self.model(X_scaled, training=True)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        # Calculate mean and standard deviation\n",
    "        predictions = np.array(predictions).squeeze()\n",
    "        mean_pred = np.mean(predictions, axis=0)\n",
    "        std_pred = np.std(predictions, axis=0)\n",
    "        \n",
    "        return mean_pred, std_pred\n",
    "\n",
    "def load_data():\n",
    "    data = {\n",
    "        'Commodity': [\n",
    "            'Rice - Basmati', 'Rice (Other than Basmati)', 'Wheat', 'Other Cereals',\n",
    "            'Pulses', 'Tobacco Unmanufactured', 'Tobacco Manufactured', 'Cashew',\n",
    "            'Cashew Nut Shell Liquid', 'Sesame Seeds', 'Niger Seeds', 'Groundnut',\n",
    "            'Other Oil Seeds', 'Vegetable Oils', 'Oil Meals', 'Guar Gum Meal',\n",
    "            'Castor Oil', 'Shellac', 'Sugar', 'Molasses', 'Fruits / Vegetable Seeds',\n",
    "            'Fresh Fruits', 'Fresh Vegetables', 'Processed Vegetables',\n",
    "            'Processed Fruits and Juices', 'Cereal Preparations'\n",
    "        ],\n",
    "        '2018-2019': [2247.34, 1577.02, 20.84, 208.83, 155.13, 291.99, 200.98, 311.77,\n",
    "                      2.44, 289.83, 5.11, 209.77, 49.37, 58.14, 591.9, 355.38, 445.41,\n",
    "                      22.27, 490.83, 27.76, 78.52, 293.03, 425.53, 150.83, 299.22, 267.44],\n",
    "        '2019-2020': [2032.1, 1021.77, 33.14, 97.22, 120.09, 274.61, 211.18, 281.3,\n",
    "                      1.55, 254.97, 5.41, 190.32, 30.07, 65.97, 430.11, 268.09, 523.11,\n",
    "                      21.7, 808.32, 36.7, 67.01, 277.68, 408.81, 152.54, 298.57, 273.45],\n",
    "        '2020-2021': [2123.58, 1964.87, 108.77, 212.99, 154.76, 264.23, 157.66, 168.72,\n",
    "                      1.03, 224.11, 12.02, 236.51, 35.35, 131.22, 461.65, 122.03, 438.01,\n",
    "                      36.91, 1364.08, 44.49, 72.09, 230.65, 440.63, 213.87, 321.88, 280.21],\n",
    "        '2021-2022': [1659.6, 2968.77, 630.15, 467.42, 135.25, 288.53, 174.91, 222.02,\n",
    "                      1.89, 188.05, 4.77, 246.42, 14.04, 99.22, 471.65, 178.85, 615.62,\n",
    "                      44.86, 1820.68, 95.4, 65.81, 301.3, 435.41, 213.77, 370.12, 314.92],\n",
    "        '2022-2023': [2279.66, 3207.29, 1487.47, 524.85, 329.55, 446.72, 212.77, 157.09,\n",
    "                      5.09, 242.87, 2.82, 245.57, 39.76, 178.9, 556.61, 343.65, 662.93,\n",
    "                      48.8, 2649.0, 127.28, 73.03, 313.46, 439.52, 254.01, 440.62, 359.31]\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def prepare_data(df):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    commodities = []\n",
    "    \n",
    "    for commodity in df['Commodity']:\n",
    "        values = df[df['Commodity'] == commodity].iloc[:, 1:].values.flatten()\n",
    "        X = values[:-1].reshape(1, -1)  # Use all but last year as features\n",
    "        y = values[-1]  # Use last year as target\n",
    "        \n",
    "        X_list.append(X)\n",
    "        y_list.append(y)\n",
    "        commodities.append(commodity)\n",
    "    \n",
    "    X = np.vstack(X_list)\n",
    "    y = np.array(y_list)\n",
    "    \n",
    "    return X, y, commodities\n",
    "\n",
    "def plot_predictions_mcdonn(df, predictions, uncertainties, commodities):\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    years = df.columns[1:].astype(str)\n",
    "    next_year = '2023-2024'\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(commodities)))\n",
    "    \n",
    "    for idx, commodity in enumerate(commodities):\n",
    "        values = df[df['Commodity'] == commodity].iloc[:, 1:].values.flatten()\n",
    "        color = colors[idx]\n",
    "        \n",
    "        plt.plot(years, values, marker='o', color=color, alpha=0.5)\n",
    "        plt.errorbar(next_year, predictions[idx], \n",
    "                    yerr=uncertainties[idx], \n",
    "                    fmt='o', capsize=5, color=color,\n",
    "                    label=commodity)\n",
    "    \n",
    "    plt.title('MCDoNN Commodity Price Predictions with Uncertainty')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_evaluation_metrics_mcdonn(df, predictions, uncertainties, commodities):\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # Get actual values (last year)\n",
    "    actual_values = df.iloc[:, -1].values\n",
    "    \n",
    "    # 1. Prediction Error Distribution\n",
    "    plt.subplot(2, 2, 1)\n",
    "    errors = predictions - actual_values\n",
    "    sns.histplot(errors, kde=True)\n",
    "    plt.title('Prediction Error Distribution')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # 2. Actual vs Predicted Scatter Plot\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(actual_values, predictions, alpha=0.5)\n",
    "    plt.plot([min(actual_values), max(actual_values)], \n",
    "             [min(actual_values), max(actual_values)], 'r--')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    r2 = r2_score(actual_values, predictions)\n",
    "    plt.title(f'Actual vs Predicted (R² = {r2:.3f})')\n",
    "    \n",
    "    # Add commodity labels\n",
    "    for i, commodity in enumerate(commodities):\n",
    "        plt.annotate(commodity, (actual_values[i], predictions[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    # 3. Uncertainty Distribution\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.histplot(uncertainties, kde=True)\n",
    "    plt.title('Uncertainty Distribution')\n",
    "    plt.xlabel('Prediction Uncertainty (std)')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # 4. Uncertainty vs Error\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.scatter(uncertainties, np.abs(errors), alpha=0.5)\n",
    "    plt.xlabel('Prediction Uncertainty')\n",
    "    plt.ylabel('Absolute Error')\n",
    "    plt.title('Uncertainty vs Absolute Error')\n",
    "    \n",
    "    # Add commodity labels\n",
    "    for i, commodity in enumerate(commodities):\n",
    "        plt.annotate(commodity, (uncertainties[i], np.abs(errors[i])), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def main():\n",
    "    # Load and prepare data\n",
    "    df = load_data()\n",
    "    X, y, commodities = prepare_data(df)\n",
    "    \n",
    "    # Create and train MCDoNN model\n",
    "    model = MCDoNN(input_dim=X.shape[1])\n",
    "    history = model.fit(X, y)\n",
    "    \n",
    "    # Make predictions with uncertainty\n",
    "    mean_predictions, uncertainties = model.predict_with_uncertainty(X)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = math.sqrt(mean_squared_error(y, mean_predictions))\n",
    "    mae = mean_absolute_error(y, mean_predictions)\n",
    "    r2 = r2_score(y, mean_predictions)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nMCDoNN Model Evaluation Metrics\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Overall RMSE: {rmse:.2f}\")\n",
    "    print(f\"Overall MAE: {mae:.2f}\")\n",
    "    print(f\"Overall R²: {r2:.3f}\")\n",
    "    \n",
    "    # Print individual predictions\n",
    "    print(\"\\nPredictions for 2023-2024:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Commodity':<30} {'Prediction':>10} {'Uncertainty':>12}\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, commodity in enumerate(commodities):\n",
    "        print(f\"{commodity:<30} {mean_predictions[i]:>10.2f} ±{uncertainties[i]:>10.2f}\")\n",
    "    \n",
    "    # Create and save visualizations\n",
    "    pred_fig = plot_predictions_mcdonn(df, mean_predictions, uncertainties, commodities)\n",
    "    pred_fig.savefig('mcdonn_predictions.png', bbox_inches='tight', dpi=300)\n",
    "    plt.close(pred_fig)\n",
    "    \n",
    "    eval_fig = plot_evaluation_metrics_mcdonn(df, mean_predictions, uncertainties, commodities)\n",
    "    eval_fig.savefig('mcdonn_evaluation.png', bbox_inches='tight', dpi=300)\n",
    "    plt.close(eval_fig)\n",
    "    \n",
    "    print(\"\\nVisualization files have been saved:\")\n",
    "    print(\"1. mcdonn_predictions.png - Shows predictions with uncertainty bands\")\n",
    "    print(\"2. mcdonn_evaluation.png - Shows evaluation metrics and uncertainty analysis\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 248\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2. transformer_attention_weights.png - Shows attention weight patterns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 248\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 200\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    197\u001b[0m X, y, commodities \u001b[38;5;241m=\u001b[39m prepare_data(df)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Create and train Transformer-MCDO model\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerMCDO\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mff_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_transformer_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\n\u001b[0;32m    207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Make predictions with uncertainty\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 88\u001b[0m, in \u001b[0;36mTransformerMCDO.__init__\u001b[1;34m(self, input_dim, embed_dim, num_heads, ff_dim, num_transformer_blocks, dropout_rate)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dim, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, ff_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_transformer_blocks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m=\u001b[39m input_dim\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mff_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_transformer_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n",
      "Cell \u001b[1;32mIn[2], line 103\u001b[0m, in \u001b[0;36mTransformerMCDO._build_model\u001b[1;34m(self, embed_dim, num_heads, ff_dim, num_transformer_blocks, dropout_rate)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Transformer blocks\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_transformer_blocks):\n\u001b[1;32m--> 103\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mff_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m)\u001b[49m(x)\n\u001b[0;32m    105\u001b[0m x \u001b[38;5;241m=\u001b[39m GlobalAveragePooling1D()(x)\n\u001b[0;32m    106\u001b[0m x \u001b[38;5;241m=\u001b[39m Dropout(dropout_rate)(x)\n",
      "Cell \u001b[1;32mIn[2], line 70\u001b[0m, in \u001b[0;36mTransformerBlock.__init__\u001b[1;34m(self, embed_dim, num_heads, ff_dim, dropout_rate)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28msuper\u001b[39m(TransformerBlock, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt \u001b[38;5;241m=\u001b[39m MultiHeadSelfAttention(embed_dim, num_heads, dropout_rate)\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m([\n\u001b[0;32m     71\u001b[0m     Dense(ff_dim, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     72\u001b[0m     Dropout(dropout_rate),\n\u001b[0;32m     73\u001b[0m     Dense(embed_dim),\n\u001b[0;32m     74\u001b[0m ])\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm1 \u001b[38;5;241m=\u001b[39m LayerNormalization(epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm2 \u001b[38;5;241m=\u001b[39m LayerNormalization(epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import math\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "class MultiHeadSelfAttention(Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8, dropout_rate=0.1):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f'embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}'\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = Dense(embed_dim)\n",
    "        self.key_dense = Dense(embed_dim)\n",
    "        self.value_dense = Dense(embed_dim)\n",
    "        self.combine_heads = Dense(embed_dim)\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        \n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        \n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        \n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "        \n",
    "        attention, _ = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        output = self.dropout(output, training=training)\n",
    "        return output\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads, dropout_rate)\n",
    "        self.ffn = Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attention_output = self.att(inputs, training=training)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TransformerMCDO:\n",
    "    def __init__(self, input_dim, embed_dim=32, num_heads=4, ff_dim=32, num_transformer_blocks=2, dropout_rate=0.1):\n",
    "        self.input_dim = input_dim\n",
    "        self.model = self._build_model(embed_dim, num_heads, ff_dim, num_transformer_blocks, dropout_rate)\n",
    "        self.scaler = MinMaxScaler()\n",
    "        \n",
    "    def _build_model(self, embed_dim, num_heads, ff_dim, num_transformer_blocks, dropout_rate):\n",
    "        inputs = Input(shape=(self.input_dim,))\n",
    "        x = Dense(embed_dim)(inputs)\n",
    "        x = Reshape((-1, embed_dim))(x)\n",
    "        \n",
    "        # Positional encoding\n",
    "        positions = tf.range(start=0, limit=self.input_dim, delta=1)\n",
    "        position_embeddings = tf.keras.layers.Embedding(input_dim=self.input_dim, output_dim=embed_dim)(positions)\n",
    "        x = x + position_embeddings\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for _ in range(num_transformer_blocks):\n",
    "            x = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)(x)\n",
    "        \n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        x = Dense(ff_dim, activation=\"relu\")(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        outputs = Dense(1)(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y, validation_split=0.2, epochs=200, batch_size=32):\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_scaled, y,\n",
    "            validation_split=validation_split,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict_with_uncertainty(self, X, n_iterations=100):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        predictions = []\n",
    "        \n",
    "        for _ in range(n_iterations):\n",
    "            pred = self.model(X_scaled, training=True)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        predictions = np.array(predictions).squeeze()\n",
    "        mean_pred = np.mean(predictions, axis=0)\n",
    "        std_pred = np.std(predictions, axis=0)\n",
    "        \n",
    "        return mean_pred, std_pred\n",
    "\n",
    "# ... (keep the load_data and prepare_data functions from the previous implementation)\n",
    "\n",
    "def plot_predictions_transformer(df, predictions, uncertainties, commodities):\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    years = df.columns[1:].astype(str)\n",
    "    next_year = '2023-2024'\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(commodities)))\n",
    "    \n",
    "    for idx, commodity in enumerate(commodities):\n",
    "        values = df[df['Commodity'] == commodity].iloc[:, 1:].values.flatten()\n",
    "        color = colors[idx]\n",
    "        \n",
    "        plt.plot(years, values, marker='o', color=color, alpha=0.5)\n",
    "        plt.errorbar(next_year, predictions[idx], \n",
    "                    yerr=uncertainties[idx], \n",
    "                    fmt='o', capsize=5, color=color,\n",
    "                    label=commodity)\n",
    "    \n",
    "    plt.title('Transformer-MCDO Commodity Price Predictions with Uncertainty')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_attention_weights(model, X, commodities):\n",
    "    \"\"\"Plot attention weights for visualization\"\"\"\n",
    "    # Get attention weights from the first transformer block\n",
    "    attention_layer = model.model.layers[4]  # Adjust index based on your model\n",
    "    attention_weights = attention_layer.att(tf.constant(X), training=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(min(4, len(commodities))):\n",
    "        sns.heatmap(attention_weights[i], ax=axes[i])\n",
    "        axes[i].set_title(f'Attention Weights - {commodities[i]}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def main():\n",
    "    # Load and prepare data\n",
    "    df = load_data()\n",
    "    X, y, commodities = prepare_data(df)\n",
    "    \n",
    "    # Create and train Transformer-MCDO model\n",
    "    model = TransformerMCDO(\n",
    "        input_dim=X.shape[1],\n",
    "        embed_dim=32,\n",
    "        num_heads=4,\n",
    "        ff_dim=32,\n",
    "        num_transformer_blocks=2,\n",
    "        dropout_rate=0.1\n",
    "    )\n",
    "    history = model.fit(X, y)\n",
    "    \n",
    "    # Make predictions with uncertainty\n",
    "    mean_predictions, uncertainties = model.predict_with_uncertainty(X)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = math.sqrt(mean_squared_error(y, mean_predictions))\n",
    "    mae = mean_absolute_error(y, mean_predictions)\n",
    "    r2 = r2_score(y, mean_predictions)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nTransformer-MCDO Model Evaluation Metrics\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Overall RMSE: {rmse:.2f}\")\n",
    "    print(f\"Overall MAE: {mae:.2f}\")\n",
    "    print(f\"Overall R²: {r2:.3f}\")\n",
    "    \n",
    "    # Print individual predictions\n",
    "    print(\"\\nPredictions for 2023-2024:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Commodity':<30} {'Prediction':>10} {'Uncertainty':>12}\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, commodity in enumerate(commodities):\n",
    "        print(f\"{commodity:<30} {mean_predictions[i]:>10.2f} ±{uncertainties[i]:>10.2f}\")\n",
    "    \n",
    "    # Create and save visualizations\n",
    "    pred_fig = plot_predictions_transformer(df, mean_predictions, uncertainties, commodities)\n",
    "    pred_fig.savefig('transformer_mcdo_predictions.png', bbox_inches='tight', dpi=300)\n",
    "    plt.close(pred_fig)\n",
    "    \n",
    "    # Plot attention weights\n",
    "    att_fig = plot_attention_weights(model, X, commodities)\n",
    "    att_fig.savefig('transformer_attention_weights.png', bbox_inches='tight', dpi=300)\n",
    "    plt.close(att_fig)\n",
    "    \n",
    "    print(\"\\nVisualization files have been saved:\")\n",
    "    print(\"1. transformer_mcdo_predictions.png - Shows predictions with uncertainty bands\")\n",
    "    print(\"2. transformer_attention_weights.png - Shows attention weight patterns\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
